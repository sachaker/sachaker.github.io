---
layout: gallery
title: Thoughts (blog space)
mathjax: true
permalink: /thoughts/
---

### *February 18th, 2022*
---
# **My secret side project**

<img style="float: center; margin: 0px 25px 25px 0px;" src="{{site.imgurl}}/blogContent/hawaii.png" height="100%" width="100%"/>


On a gorgeous Hawaiian day in late December I whipped together a little script. For various reasons, I am keeping the purposes of this script secret. What you need to know is that this little script was birthed from the confluence of my boredom and my randomly reading API docs of a popular mobile app. I thought of a cool feature one could build using the API and built out a pretty basic service in a few hours. I sent my MVP to a close friend in consumer social. 10sec later he texts back...

<img style="float: left; margin: 0px 25px 25px 0px;" src="{{site.imgurl}}/blogContent/sideproject_imessage.png" height="35%" width="35%"/>

He gives me the idea for an improved derivative of this product and suggests we productize and ship that new product in the next 2 weeks. We partner up with one of his colleagues to build out a website and productionize my little MVP.

I whip together a sufficiently scalable backend and clean up my script. My friend designs a simple website which his colleague builds with a basic API layer to interface with our db. We reach out to some TikTokers to post and within a day we’ve locked in the campaign.

After some basic stress tests, we feel we are ready and the first TikTok post goes out on Jan. 16th. Within the first 22 days we get the following data:

<img style="float: left; margin: 0px 25px 25px 0px;" src="{{site.imgurl}}/blogContent/sideProject_stats.png" height="400px"/>

TL;DR this project is still in its infancy and we initially got obliterated by the demand (crashing servers, blocking/delaying services, etc..) We have a lot of conviction in the project and are going to start properly ramping up the growth. This was all just a very prelimenary test-run.

I’ve realized in pursuing this side project the power of rapid exploration. This flies in the face of what most founders are taught to believe. People in tech overindex on the merits of shipping products the conventional way (i.e. come up with an idea, make a pitch deck, reach out to investors, build out an MVP after hiring the 10x engineers you convinced to join, tell everyone on LinkedIn you’re the CEO of a future unicorn despite the fact that all you’ve done thus far is incorporate with Stripe Atlas, etc..) Even as kids we are trained to think so definitively about things—”I *will* be an X one day”. Why not just give it a shot and reassess in a couple of weeks?

This approach is not per se a prescription for anyone considering pursuing a startup but moreso a recommendation to those who are capable enough to build products and curious enough to indulge that capability. Just build something casually for 2 weeks and make sure to get it to market fast enough to determine whether it’s worth pursuing further.

If you are toying with an idea, I suggest you:

1. Quickly (1 week MAX) build an MVP (if you are technical—if not, then make a Figma protoype)
2. Text the absolute minimum number of people you need to see this project through to completion to collaborate with you (2 other people MAX, in my opinion)
3. Make a compelling landing page with at least a sign up field
4. Launch using TikTok and make an assessment in 2 weeks as to whether the product is worth pursuing further (spend no more than $2,000)

The opportunity cost is completely overshadowed by the potential upside for these side projects.  I want to advocate heavily for lowering the mental friction of starting a project. Start putting together something cool tomorrow and let me know how it goes (DM’s open on [Twitter](https://twitter.com/sachaker)!)


&nbsp; 

### *March 23rd, 2021*
---
# **Change vs. Progress**

It is incredibly important not to conflate change with progress. 

Progress exists at the intersection between change (state B ≠ state A) and growth (some optimization towards the objective). Change can exist without growth (e.g. a new presidency, binary fission of a bacterium, etc.), but growth cannot exist without change (some sense of modification is requisite for optimization).

Without a driving force towards growth, the action or change will be what we can call "yieldless". For example, if I choose to go running, the act of running itself may in fact be the objective, and that will hold under any context in which I am running. However, if my goal has shifted to using running as a means of achieving something else (like moving from point A to point B), then the actions that progress me towards that goal become contextual—running on a treadmill achieves the same action without any progress (as defined by the context). By the same token, science has the bad habit of feigning progress where, in most instances, we are simply just running in place.<sup>1</sup>

It is very easy to see changes within science and be excited by the carrot of progress, but how have those changes progressed us towards the initial objective? In medicine, as an admitted generalization, the objective function is easily defined: are we closer to curing the disease in question? Without proper heuristics for determining this, yiedless changes can safely hide under the veil of progress.<sup>2</sup>

I suspect that we would benefit greatly from inoculating ourselves to the allure of change and no longer misconstruing it as progress. Who would have thought that using the ultimate goal to guide the action could be so hard??

&nbsp; 
> ##### <sup>1</sup>The iPhone, as another example, has arguably undergone many yieldless changes over the years (compare the growth rate of the product quality in the early years of the iPhone to now), yet the illusion of continued progress has been upheld by the consistency of these iterations.

> ##### <sup>2</sup>Collison and Cowen's 2019 piece on this ([here](https://www.theatlantic.com/science/archive/2019/07/we-need-new-science-progress/594946/)) outlines the urgency of establishing these heurestics across industries. It's a great read.

&nbsp;

### *Novemeber 3rd, 2020*
---
# **Reflexivity in politics**
Distributing poll results to a population before an election degrades the predictiveness of that population by a significant margin. Is there a [psephological](https://en.wikipedia.org/wiki/Psephology) law for this? I'm not sure, but there should be...

In 2016, I warned my family and friends that distributing polls that predict a landslide in Clinton's favor would result in mass complacency, and that this complacency would manifest as poor Democractic turnout. If my community of friends is at all a microcosm to the younger population in the United States, in 2016, ~10 of them (likely more—I only asked a small handful) did not vote because "Hilary [was] going to win in a landslide... No need to go through the pain of voting". 3 of those 10 friends were from Florida, a [crucial swing state](https://en.wikipedia.org/wiki/2016_United_States_presidential_election_in_Florida) in that election. I fear that the same thing may happen with this election today.

While I am not confident in the final outcome of this election, I do feel comfortable positing that the final result will be *much* closer than predicted by any leading pollsters.

The point here is that I think that there is a lot about [reflexivity](https://en.wikipedia.org/wiki/Reflexivity_(social_theory)) that remains unexplored and believe that finding elegant ways to mathematically model reflexivity in different systems would be of immeasurable benefit to statistics, economics, neuroscience, and beyond.

> ##### Shockingly, the "error" of the pollsters' predictions in 2016 was actually pretty much at the [mean combined error for past elections](https://fivethirtyeight.com/features/the-polls-are-all-right/) (only 4% higher than the average). It should be noted that these data are weighted by the number of polls put out by each institution and so the most active pollsters are weighted the highest. The interesting insight here would be to see whether there is a correlation between this weight and the error itself, since the most popular pollsters can be expected to be covered by big media outlets more than their counterparts. The more media coverage a poll receives, the less likely it is to be correct.

&nbsp;


### *June 1st, 2020*
---
# **Some questions...**

> ##### A few random things that I've been thinking about quite a bit. Some date back years, others in the past few hours

#### Why does it feel like Hungary disproportionately produced some of the brightest and most prolific minds of the 20th century?
- [Erdos](https://en.wikipedia.org/wiki/Paul_Erd%C5%91s), [von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann), [Gabor](https://en.wikipedia.org/wiki/Dennis_Gabor), [Wigner](https://en.wikipedia.org/wiki/Eugene_Wigner), [Elo](https://en.wikipedia.org/wiki/Arpad_Elo), and [many, many more](https://en.wikipedia.org/wiki/List_of_Hungarians)

#### How can we quantify the changes to a predicted outcome after that prediction is disseminated to the population whose behavior is being predicted?
- Think: election polls, distributions of wait times, etc. Is this sufficient to describe the glaring errors of all the 2016 US Presidential polls?

#### What city in a Third World country will bear the greatest intellectual fruits in the next 25 years? 
- Lagos? CDMX? 

#### Why has the wash time of restaurant-grade dishwashers (~1min) been so disparate from its domestic counterpart for so long? 
- I'm not convinced that this "industry-domestic discrepancy" exists to the same extent for other applicable technologies, especially not for so long (the processor in your iPhone is 5 orders of magnitude more powerful than that of Apollo 11)

#### Is globalization always a good thing? And to what extent does the globalization of technology align with (and also engender) cultural globalization?
- China has reaped the rewards of Western innovation without the "burden" of democracy (great convo on this [here](https://podcasts.apple.com/us/podcast/34-zev-weinstein-on-parenting-boys-generation-z/id1469999563?i=1000474506222)), yet it does not feel that this adoption has been at the cost of any cultural identity. On the other hand, the physical presence of Turks in Germany has had profound influence on the culture (and cuisine) of the latter, though their relationship dates back to at least WWI (before US-China relations) so maybe the comparison isn't fair.

&nbsp;


### *May 9th, 2020*
---
# **My take on the COVID-19 market**

> ##### Disclaimer: I am not recommending any specific stock purchases here, nor am I licensed to do so. I merely want to share the thought processes that helped me profit during the volatility effectuated by COVID-19


People have been scared.


It makes sense, how could you not be when there exists the incessant echo chamber that is contemporary media? Call it ignorance, but I usually never listen to stock recommendations or follow markets through any particular media outlet. My investment approach could not be simpler: I think about the market, I substantiate or uproot my hypothesis with market data, and update my internal model of that market using these data.


A reasonable response would be to laugh. *What the hell do you know, punk?* Well, not much, but enough to consistently beat out some of the weirdest markets in the past 3 years and hold a constant >30% returns for that time. I have helped my family, other families, and my friends invest in the market, because at its best, it's a lot of fun. 


I'm no guru, I just love contrarian thinking. Thankfully, this translates well in investing: flying in the face of conventional wisdom, in late 2018, I shorted FB and AAPL based on a theory I had about their self-imposed pigeon holing. If this sound stupid, I encourage you to check out their stock prices for that year (P.S. I made >350% returns from those puts). Big money is made when you are right and the majority is wrong (see [Burry](https://en.wikipedia.org/wiki/Michael_Burry), [Dalio](https://en.wikipedia.org/wiki/Ray_Dalio), or [Soros](https://en.wikipedia.org/wiki/George_Soros)). If everyone says I am wrong about a prediction, it's often an incentive to chase the idea further.


Back to 2020... COVID-19 has taken a huge chunk out of the economy, a chunk which is almost perfectly distributed across each industry. On March 10th, I was debating with my mom about this investment climate, as she was consumed by anxiety about the stock market and how the experts were saying it would be a recession (I remain pretty convinced that an imminent threat of such magnitude to the economy that's universally accepted as likely will in fact beget solutions that prevent it from actually materializing. These things really are reflexive in that way and every major economic recession has blindsided the majority, that's why they happen! That's probably for another blog post, though).


My mom, in line with the rest of America, was convinced that everything would plummet and that it would be a year for any of these stocks to rebound. I thought this all over... Just looking at the tech industry, it would make sense that hardware companies would be severely impacted—a lot of these tech behemoths got a cold splash of water exposing their overdependence on China (as if the [ongoing trade wars](https://en.wikipedia.org/wiki/China%E2%80%93United_States_trade_war) weren't enough). These hardware companies (e.g. AAPL, NVDA, AMD) now have to rethink their entire supply chain. The software companies (e.g. TWLO, OKTA, SHOP), however, should be much more nimble and unaffected by the downturn, right? Teams of programmers should still be able to deploy code from their bedrooms and people should still consume their products remotely.


As it turned out, the aforementioned industries (and individual stocks) were affected equally, and I just knew that the software stocks would rebound earlier. I'll let the data speak for themselves:


<img style="float: left; margin: 0px 15px 15px 0px;" src="{{site.imgurl}}/stocks.png"/>


<p align="center">
	<b>Stock returns from 3/10/20 - 5/9/20 (green = software, red = hardware)</b>
</p>


Now before you think that I retroactively changed my selection of stocks to sound smart in a blog post, these were the actual 3 that I recommended. I should probably say that I'm still invested in TWLO, and have been for years (first bought at $76/share), but I sold off all my other stocks about 5 months ago when it became too time consuming.


These differences may seem slight, but the worst performing of my recommendations is 134% of the biggest return for the aforementioned hardware companies (and TWLO returns are 519% of AAPL returns for this period!).


The point here is that intuition and calculated thought can be an important factor in distinguishing you in investment. Don't believe me? Billionaire George Soros literally moves millions of dollars [based on pain](https://www.equitymaster.com/diary/detail.asp?date=01/13/2016&story=1&title=George-Soros-Has-Got-a-Backache-Again-and-This-Time-Its-Because-of-China) in different parts of his body. Everyone else uses financials and the same old indicators—be different.


&nbsp;



### *April 19th, 2020*
---
# **The crucial missing link for Artificial Neural Networks**

> ##### I'm going to jump right into this one, so apologies if it lacks the technical introduction such a post might demand


Artificial Neural Networks (ANNs), as the name might suggest, were built with the brain as a primary influence—artificial neurons connect to each other, each conferring signals to others and each with connections that vary mostly in accordance with the alignment between the neuron's activity and the ultimate achievement of the system's goal (see [objective functions](https://en.wikipedia.org/wiki/Mathematical_optimization)). This was a breakthrough in the field of computer science as it completely transformed the capabilities of computers to achieve complex tasks. Computers were no longer just efficient computing devices that did simple tasks quickly and consistently—they now could *improve* iteratively (see [Perceptron](https://en.wikipedia.org/wiki/Perceptron) and the lesser appreciated [MENACE](https://www.mscroggs.co.uk/blog/19)).


Things have improved a lot with regards to computing power since those early days, but the reality is that the fundamentals of ANNs simply haven't witnessed the same growth.


What we see is that, despite the fact that we can reach unprecedented [network depth](https://towardsdatascience.com/gating-and-depth-in-neural-networks-b2c66ae74c45), performance just doesn't scale. Part of this is due to what is dubbed the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), in which the changes made to the connections of neurons deep into the network are negligible and therefore less likely to contribute to the final network output. This problem was significantly alleviated by the development of [ResNets](https://arxiv.org/abs/1512.03385), the original version of which broke numerous world records for image classification and other tasks. However, increasing the depth of this network still results in a performance plateau at a certain depth (the error difference between 50 and 100 layer ResNets, for the tasks I've used them for, is negligible). How has increasing the number of neurons, connections, and layers to unprecedented heights in our ANNs not brought us within reasonable proximity to the cognitive powers of the brain?


Take a look at how performance of a basic network can actually get worse by adding layers (image classification network on CIFAR-10 dataset):


<p align="center">
	<b>Deeper version of network does worse at image classification</b>
</p>


<img style="float: left; margin: 0px 15px 15px 0px;" src="{{site.imgurl}}/blogContent/errorXiterXlayers.png" height="250%" width="250%"/>


It's clear that a network with connectivity as its only parameter is insufficient to generate the flexibility of the brain.


I believe that the most significant factor missing from the way we build ANNs that is present in the brain is electric fields. While artifical neurons in ANNs are mathematical representations, biological neurons (just "neurons", from now on) occupy physical space, and thus their interactions are influenced along a spatial dimension (*Note: I definitely do not consider layers of an ANNs as spatial*). I also believe that the importance of the electric fields emitted by neurons is severely underappreciated in the field of neuroscience, despite the vast evidence for their significance in the biophysics of the brain as well as cognition ([here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4534030/) and [here](https://www.scientificamerican.com/article/brain-electric-field/), and I'm comfortable arguing that the breakthrough of [LFPs](http://www.scholarpedia.org/article/Local_field_potential) is because they register both action potentials and the resulting electric fields).


The electric fields emitted by active neurons contribute to the complexity of the entire system by being reflexive—an active neuron emits a field which in turn influences the firing dynamics of that neuron and the surrounding neurons, which influence downstream neurons which in turn emit electric fields which in turn... you see where this going. The result is that groups of neurons in the same vicinity will collectively generate large electric fields that will modify their biophysical signature and possibly enable computations that are otherwise unachievable in a state of individual isolation. The fact that neurons are exist in the world of biology has been seen as a hindrance to their computational abilities (slow time constants, high noise, energetically expensive, etc.), but the reality is that such complexity simply could not exist on a processor and instead *only in a biological system*, or one that merges the two.


Anyway, the point here is that for ANNs to approximate or surpass the complexity and cognitive flexibility of the brain, they must first address this issue. I think embedding actual neural tissue in circuitry ("wetware") is a really smart way of tackling this, and I'll be keeping my eye on [Cortical Labs](https://www.cclabs.ai/), [Koniku](https://koniku.com/technology), and maybe even [this nutjob](https://www.youtube.com/watch?v=h2nNKbO9-Eg) (who did the inverse).


&nbsp;


### *April 10th, 2020*
---
# **Humanity has already found the neural seat of consciousness, science has just overlooked it**

If someone put a gun to my head and made me blurt out the part of the brain that houses consciousness<sup>†</sup>, I'd say "thalamus" without breaking a sweat.

[The thalamus](https://www.ncbi.nlm.nih.gov/books/NBK542184/) is a chunk of gray matter found in the forebrain. This is the Grand Central of the brain—it receives input from a vast number of areas and its projections innervate almost all sections of the cerebral cortex. The result: the thalamus both integrates and distributes information from countless areas. For instance, if sensory input comes in, the thalamus works its magic (transforms that sensory information along some dimension), and then spits it back into the sensory area from whence it came.

I believe quite strongly that anticipation is an evolutionary prerequisite for consciousness and that it demands the coordinated integration of the senses. Why does anticipation require the integration of the senses? Just think about it: let's say a person is swinging a punch at you. Well, first of all, you would need to calculate the trajectory of their swing ([motor cortex](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0191480)) which would require that you visualize their fist reaching your face ([visual cortex](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4595480/)). Significance is assigned to avoiding their fist reaching your face because you know that it will hurt ([nociceptors](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2964977/)). All of this activity coming from disparate regions of your brain before the fist has even come near your face. Without the tight and timely integration of all the information, you can bet your ass that tomorrow you'd wake up with a black eye... 

Why is anticipation a requisite for consciousness? Because anticipation itself requires:
- An internal, updatable model of the world (think: [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation))
- A sense of futurity (not just living in the *now*, but in the *next*)
- Pattern identification (I would feel pretty comfortable positing that this is the product of the other two)

I am putting forward that these 3 pillars are the building blocks on which consciousness is built. If we could find regions in the brain that encode or enable these, then I'd say we are well on our way to uncovering the secrets of consciousness.

Back to the thalamus... Just based on sheer connectivity, it seems like the thalamus has always been a prime candidate for understanding neural processes on a fundamental level. Why has it taken so long for focus to shift to the thalamus? The thalamus has historically been viewed as a "low-level" (yuck!) center (yuck yuck!!), as part of a contrived framework of anatomical hierarchy in the brain. Everyone learns in their 101 courses that "the midbrain is the lower brain... and the thalamus and associated areas are part of the limbic system" ([here](https://science.howstuffworks.com/life/inside-the-mind/human-brain/brain5.htm), [here](https://courses.lumenlearning.com/teachereducationx92x1/chapter/lower-level-structures-of-the-brain/), [here](https://www.psychologytoday.com/us/blog/where-addiction-meets-your-brain/201404/your-lizard-brain), and [here](https://www.khanacademy.org/science/health-and-medicine/executive-systems-of-the-brain/emotion-lesson/v/emotions-limbic-system)). The fact that they peddle this crap to unwitting freshmen is infuriating! "Experts" loooove to talk about the limbic system, also called your "Lizard Brain" (yes, experts literally call it that). These experts have grouped the thalamus in this category based on the antiquated idea that neural anatomy recapitulates the evolutionary development of the brain. In plain English: the deepest parts of the brain are the ones most like our evolutionary ancestors. As an unfortunate consequence of this antiquated school of thought, the thalamus has unfairly been identified as a primitive area, with little historical investigation into its role in cognition! Individuals and scientific communities are influenced deeply by egos and the perception of fundamental truths—scientists are unwilling to accept novel discoveries if they fly in the face of what the scientists fundamentally believe to be true (or if they spent most of their life committed to that which is being overturned). This has largely stifled investigation into the thalamus and acceptance of the promising results that come out of it.

[Mike Halassa](http://halassalab.mit.edu/) at MIT is one of the few scientists who gets it. He's still relatively early in his career but I am willing to predict that he will be one of the pioneers of his generation that will unhinge our current understanding of the brain (see also [Doris Tsao](https://en.wikipedia.org/wiki/Doris_Tsao) and more recently, [Steve Ramirez](http://theramirezgroup.org/team/steve-ramirez)). If you're looking for a good read on the thalamus and cognition, Halassa's paper [Schmidtt et al. 2017](https://www.nature.com/articles/nature22073) is seminal. In brief, the paper demonstrates that thalamic neurons engage in task-relevant computations that freely transition from object-based to task-based. This means that the thalamic neurons can encode the identity of specific stimuli *as well as* their significance. Importantly, these thalamic neurons were found to regulate activity in the pre-frontal cortex (PFC), which is largerly seen as the most significant center in cognition. This means that the "low-level" area was not merely relaying basic sensory information from the PFC, but in fact was responsible for conferring its cognitive flexibility!

Mike and I had a great discussion last November about this fundamental oversight in the field and how overwhelming the evidence is for the thalamus' role in cognition. The ingredients for the thalamus coming out as the neural seat of consciousness are all there—it integrates and distributes information from the broadest reaches of the brain, it confers the "high-level brain" the ability to transition between definitions of identity (object --> significance), and has been overlooked in the field for reasons unrelated to its significance in the brain (no one has gotten far trying to find consciousness in the brain, so maybe they should start looking under their noses!). I am super excited to know that this fundamental transition in the field of neuroscience is imminent and will be keeping a close eye on it all from afar.


&nbsp; 
><sup>†</sup>Defining consciousness is not something I will dive into here, despite some implicit definitions. That's an unproductive rabbit hole to be avoided when trying to identify candidate neural substrates for consciousness

>**Bonus:** Google Trends for "[thalamus](https://trends.google.com/trends/explore?date=2007-04-09%202020-04-09&geo=US&q=thalamus)" (I promise this won't be my only data source) shows an incredible periodicity with annual maxima every October going back to 2007. Can you guess why? :)

&nbsp;


### *April 8th, 2020*
---

# **Brown Gold: How our poop is the most untapped gold mine of the 21st Century**

Poop, and by extension, wastewater, is a data source that will be replenished each minute of every hour of every day until the end of civilization. The microbiome is *severely* underappreciated in medicine and health<sup>†</sup>.
I've always been puzzled by this... The evidence supporting the significance of the microbiome is almost insurmountable ([Alzheimer's](https://www.nature.com/articles/s41422-019-0227-7), [longevity](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6051225/), [obesity](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5082693/), [autism](https://www.nature.com/articles/d41586-020-00198-y). The list goes on...), so how the hell could this be almost universally overlooked?

Most recently, [researchers discovered](https://www.nature.com/articles/d41586-020-00973-x) that sewage water serves as a useful proxy for the extent of COVID-19 infections in the community. This made me wonder: why haven't we done this before?


What we have is a fundamental misalignment in the current and actual values of this asset (poop). Call it arbitrage or just plain old opportunity—the reality is clear. Wastewater is (literally) the shit that no one wants (well, [a few people](https://www.usgs.gov/special-topic/water-science-school/science/wastewater-treatment-water-use?qt-science_center_objects=0#)), and so starting to collect mass samples of wastewater wouldn't really perturb the existing system too much.


Despite all this, public interest in the microbiome seems to be stagnant:


<img style="float: left; margin: 0px 15px 15px 0px;" src="{{site.imgurl}}/blogContent/googleTrends.png" height="250%" width="250%"/>


###### Okay, these might not be the most convincing data, but it's a start... Clearly the public appreciates the importance of health and bacterial diversity, but why don't people attend to the microbiome as an extension of that?


This space is going to make a select few *a lot* of money. The factors for success are all there: a paucity of competition, an almost infinite abundance of data, simple means of data collection, and a demonstrated role in domains relevant to health and disease... These are fertile grounds for discovery and the intrepid pioneers that get in early will no doubt be rewarded.

*Let not thy wastewater be wasted*... You heard it here first.


><sup>†</sup>I can name a couple good companies that recognize this systemic short-sightedness: [Pendulum Therapeutics](https://pendulum.co/) and [Gingko Bioworks](https://www.ginkgobioworks.com/). For the interested reader, I also recommend looking into [Richard Sprague](https://richardsprague.com/microbiome/), who has a pretty extensive passion project on his own microbiome

&nbsp; 
---

##### Thoughts on any of these articles (whether agreeable or otherwise)? Shoot me an [email](malito:sacha@nyu.edu)


&nbsp; 
&nbsp; 

<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fbfbfb;; clear:left; font:14px Helvetica,Arial,sans-serif; }
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://github.us19.list-manage.com/subscribe/post?u=baf7a0905263a7207338e7584&amp;id=be11952091" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<label for="mce-EMAIL">If you want to follow my blog...</label>
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_baf7a0905263a7207338e7584_be11952091" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
